{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Itzultzailea probatzeko.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty6cyXc8_ynH",
        "colab_type": "text"
      },
      "source": [
        "Google Colab-en exekutatuz gero, aktibatu GPUaren erabilpena (Entorno de ejecución -> Cambiar tipo de entorno de ejecución -> Acelerador por hardware -> GPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-2Ez5NZ5qlL",
        "colab_type": "text"
      },
      "source": [
        "Jarri hemen \"parametroak.pt\" eta \"bpe_hirurak.model\" gorde dituzun karpetaren helbidea:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iod60TRZ5Mun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "helbidea = 'drive/My Drive/Colab Notebooks/HACOSDatuak'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__YOTZYq0P_4",
        "colab_type": "text"
      },
      "source": [
        "# Google Colab-en exekutatuz gero:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZwrTHY_0X97",
        "colab_type": "text"
      },
      "source": [
        "Colab-etik probatzeko, liburutegi batzuk instalatu beharra dago. Exekutatu hurrengo gelaxkak."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD9uYxCD1OIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "daa928e7-b8eb-4154-8cfc-d979f0e6f6a0"
      },
      "source": [
        "!pip install youtokentome\n",
        "!pip install pyonmttok"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n",
            "Installing collected packages: youtokentome\n",
            "Successfully installed youtokentome-1.0.6\n",
            "Collecting pyonmttok\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/fc/aaa5096a948f2923d5e012409586274956368e00a6a4008412fb2807882d/pyonmttok-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pyonmttok\n",
            "Successfully installed pyonmttok-1.18.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5pDPV8fyUvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5e11c87-6ff3-474e-f303-4c1800938eb6"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-10.1'\n",
        "\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "#!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 7431, done.\u001b[K\n",
            "remote: Total 7431 (delta 0), reused 0 (delta 0), pack-reused 7431\u001b[K\n",
            "Receiving objects: 100% (7431/7431), 13.90 MiB | 18.85 MiB/s, done.\n",
            "Resolving deltas: 100% (5024/5024), done.\n",
            "/content/apex\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-55_q6ukb\n",
            "Created temporary directory: /tmp/pip-req-tracker-dfn5bvq6\n",
            "Created requirements tracker '/tmp/pip-req-tracker-dfn5bvq6'\n",
            "Created temporary directory: /tmp/pip-install-xdnjejt2\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-ty41_mn1\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-dfn5bvq6'\n",
            "    Running setup.py (path:/tmp/pip-req-build-ty41_mn1/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-ty41_mn1/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-ty41_mn1/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-ty41_mn1/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-ty41_mn1/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-ty41_mn1/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-ty41_mn1/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-ty41_mn1/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-ty41_mn1 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-dfn5bvq6'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-w_md950i\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-w_md950i\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-ty41_mn1/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-ty41_mn1/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-w_md950i --python-tag cp36\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.6.0+cu101\n",
            "\n",
            "\n",
            "  /tmp/pip-req-build-ty41_mn1/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-w_md950i/apex-0.1-cp36-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v2.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam_v3.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp36-none-any.whl size=192848 sha256=b12226076439267288c50cbaac8d89852d229f23332748fb0a38d08b66edc3b1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-55_q6ukb/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-ty41_mn1\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-dfn5bvq6'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-glcv70U1fW_",
        "colab_type": "text"
      },
      "source": [
        "# Exekutatu hauek guztiak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXAUoD2M1qiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import time\n",
        "\n",
        "from apex import amp\n",
        "import youtokentome as yttm\n",
        "import pyonmttok\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dICoQbK2Ad_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad = 0\n",
        "sos = 2\n",
        "eos = 3\n",
        "\n",
        "max_seq_len = 120\n",
        "vocab_size = 20000\n",
        "d_model = 512    # Hitza adierazteko bektoreen luzera\n",
        "N = 6            # Geruza kopurua\n",
        "heads = 8        # Attention-head kopurua\n",
        "\n",
        "#batch_size = 90\n",
        "batch_size_val = 50\n",
        "\n",
        "fp16 = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-wZnIAbq4my",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K99OLL1FqkDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # Orain dagoen moduan, max_seq_len aldagai globalak definituta egon\n",
        "        # behar du\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "                \n",
        "        pe.unsqueeze_(0) # Lista -> [Lista]. in-place egiteak ezer aldatzen du?\n",
        "        # batch_size dimentsioa gehitzeko\n",
        "        # Zergatik tentsoreekin bai eta zenbakiekin (d_model) ez?\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Hasierako embedding-ak pisu handiagoa izateko, suposatzen da:\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        seq_len = x.size(1)\n",
        "        # Bi adibideetan hemen Variable torch.tensor-en ordez:\n",
        "        # x = x + torch.tensor(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
        "        #print(\"Size x: \" + str(x.size()))\n",
        "        #print(\"Size pe: \" + str(self.pe.size()))\n",
        "        x = x + self.pe[:,:seq_len] # <- Hau bakarrik eginda zer aldatzen da?\n",
        "        # Bai beste adibidean eta honen GitHub-eko kodean: \n",
        "        # return self.dropout(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Q3yHDQRLHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        # Suposatzen da tamainak ez duela zertan hori izan behar, baina\n",
        "        # multi-head izateagatik kalkulu gehiago egitea saihesten du\n",
        "        self.h = heads\n",
        "        \n",
        "        # Attention head guztiak batera:\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)     \n",
        "        self.out = nn.Linear(d_model, d_model)  # Artikuluko WO matrizea\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        # Encoder-etan, q, k eta v-tik gauza bera iritsiko zaio, \n",
        "        # sarrera embedding-arekin (edo aurreko geruzako irteera). \n",
        "        # Decoder-etan, 1.an, irteerako aurrekoak hiruretatik (edo aurreko \n",
        "        # geruzakoa).\n",
        "        # 2.ean, q-tik 1.aren emaitza, eta k eta v-tik encoder-aren irteera.\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # Buru guztiak elkartuta k, q eta v lortu, eta gero buruak banatu\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # sl = sentence length\n",
        "        \n",
        "        # transpose to get dimensions bs * h * sl * d_k\n",
        "        # Artikuluan d_k beharrean d_model jartzen du\n",
        "       \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        # Dimentsioak: bs*h*sl*d_k\n",
        "        \n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
        "        # Uste dut reshape() egitea edo contiguous() + view() berdina dela\n",
        "        \n",
        "        # Dimentsioak = bs*sl*d_model (d_model = d_k*heads delako, \n",
        "        # bestela handiagoa izan zitekeen)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "        \n",
        "        # Dimentsioak = bs*sl*d_model\n",
        "    \n",
        "        return output"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ziy35eSfSW7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    # Lehenengo 2 dimentsioak, bs eta h, independente mantentzen dira\n",
        "    # Beste 2ekin, sl eta d_k, matrize-biderketa normala, \n",
        "    # batch eta head bakoitzean. Emaitza: bs*h*sl*sl, hitz bakoitzeko esaldiko \n",
        "    # beste hitz bakoitzari eman beharreko \"garrantzia\".\n",
        "    \n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1) # Head-i dagokion dimentsioa sortzeko\n",
        "        #print(\"Maskara \" + str(mask.size()))\n",
        "        #print(\"Scores \" + str(scores.size()))\n",
        "        #print(mask)\n",
        "        scores = scores.masked_fill(mask == 0, -1e4)\n",
        "        #print(\"Scores: \")\n",
        "        #print(scores)\n",
        "        # Maskaran dagokion balioa 0 denean, mask == 0 True (1), \n",
        "        # -1e9 jartzen du, -1 000 000 000 (-inf 'illustrated' artikuluan)\n",
        "        # Zergatik? Nonbaitetik etor daitezke negatiboak?\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    # softmax azkeneko dimentsioan zehar\n",
        "    \n",
        "    #print(\"Scores softmax: \")\n",
        "    #print(scores)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    # Dimentsioak: bs*h*sl*d_k\n",
        "    \n",
        "    #print(\"Output: \")\n",
        "    #print(output)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQQ6PumDEav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Posizioka exekutatuko da. Posizio desberdinek parametroak partekatuta,\n",
        "# baina geruza desberdinek ez\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # d_ff = ezkutuko geruzaren tamaina, originalean 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbXVxdymOQGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model # forward-en ez da erabiltzen. Zertarako gorde?\n",
        "        # Normalizazioa doitzeko parametroak. Zergatik?\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True) # dimentsioak = bs*sl*1\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        norm = self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuSuRjScmR4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK8nHCq3mfGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        # Zergatik normalizazioak hor eta ez artikuluan bezala?\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHaKLhJmqSB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DualDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        self.norm_4 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        self.dropout_4 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_3 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "  \n",
        "    def forward(self, x, e1_outputs, e2_outputs, src1_mask, src2_mask, trg_mask):\n",
        "          # e1_outputs: SRC encoder-aren irteeera\n",
        "          # e2_outputs: MT encoder-aren irteera\n",
        "          # src1_mask: SRCko esaldien maskara\n",
        "          # src2_mask: MTko esaldien maskara\n",
        "          x2 = self.norm_1(x)\n",
        "          x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "          x2 = self.norm_2(x)\n",
        "          x = x + self.dropout_2(self.attn_2(x2, e2_outputs, e2_outputs,\n",
        "          src2_mask))\n",
        "          x2 = self.norm_3(x)\n",
        "          x = x + self.dropout_3(self.attn_3(x2, e1_outputs, e1_outputs,\n",
        "          src1_mask))\n",
        "          x2 = self.norm_4(x)\n",
        "          x = x + self.dropout_4(self.ff(x2))\n",
        "          return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRsP6XaNeBa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SharedEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.pe = PositionalEncoder(d_model) # Ez dio max_seq pasatzen\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        # src Embedder-etik pasata jaso behar du\n",
        "        x = self.pe(src)\n",
        "        for i in range(N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "      \n",
        "class SharedDualDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DualDecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e1_outputs, e2_outputs, src1_mask, src2_mask, trg_mask):\n",
        "        # trg Embedder-etik pasata jaso behar du\n",
        "        x = self.pe(trg)\n",
        "        for i in range(self.N):\n",
        "            #print('2a:', i, torch.cuda.memory_allocated(0))\n",
        "            x = self.layers[i](x, e1_outputs, e2_outputs, src1_mask, src2_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuB8xQ6NDBHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtfnLWURfQjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SharedDualTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.embed = Embedder(vocab_size, d_model)       \n",
        "        self.encoder = SharedEncoder(vocab_size, d_model, N, heads)\n",
        "        self.decoder = SharedDualDecoder(vocab_size, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, src1, src2, trg, src1_mask, src2_mask, trg_mask):\n",
        "        emb = self.embed(src1)\n",
        "        e1_outputs = self.encoder(emb, src1_mask)\n",
        "        emb = self.embed(src2)\n",
        "        e2_outputs = self.encoder(emb, src2_mask)\n",
        "        emb = self.embed(trg)\n",
        "        d_output = self.decoder(emb, e1_outputs, e2_outputs, src1_mask, src2_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9x1YWj8Bf8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=True, case_markup=True, soft_case_regions=True)\n",
        "\n",
        "destokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=True, \n",
        "                                    case_markup=True, soft_case_regions=True)\n",
        "\n",
        "def tokenizatu_str(lerroa):\n",
        "    tokens, _ = tokenizer.tokenize(lerroa)\n",
        "    lerroa_tok = ' '.join(tokens)\n",
        "    lerroa_tok = lerroa_tok.replace('｟mrk_case_modifier_C｠', '｟C')\n",
        "    lerroa_tok = lerroa_tok.replace('｟mrk_begin_case_region_U｠', '｟B')\n",
        "    lerroa_tok = lerroa_tok.replace('｟mrk_end_case_region_U｠', '｟E')\n",
        "    return lerroa_tok"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK897fOC2AMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ordenatzeko(elem):\n",
        "    return len(elem[0])\n",
        "\n",
        "def sartu_padding(esaldia, luzera):\n",
        "    return esaldia + (luzera-len(esaldia))*[pad]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATaAKfCpVcL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def itzuli_beam(model, src, k, b_s):\n",
        "    \n",
        "    hasi = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "   \n",
        "        src = [(esaldia[0], esaldia[1], i) for i, esaldia in enumerate(src)]\n",
        "        src.sort(key=ordenatzeko)\n",
        "        idazteko = len(src) * [\"\\n\"]\n",
        "        \n",
        "        # encoder-erako eta decoder-eko 1. posiziorako tamaina:\n",
        "        batch_size_enc = b_s // k\n",
        "        \n",
        "        #for j in range(0, len(src), batch_size_enc):\n",
        "        berriro_saiatu = False\n",
        "        j = 0\n",
        "        while j < len(src):\n",
        "            \n",
        "            print(torch.cuda.memory_allocated(0))\n",
        "\n",
        "            if berriro_saiatu and batch_size_enc > 1:\n",
        "                batch_size_enc //= 2\n",
        "                #if batch_size_enc == 0:\n",
        "                #    print(\"Ezin da jarraitu.\")\n",
        "                #    idatzi(idazteko, pe_fitx)\n",
        "                #    return\n",
        "            \n",
        "            print(\"{}. esalditik aurrera itzultzen...\".format(j+1))\n",
        "\n",
        "            batch = src[j : j+batch_size_enc]\n",
        "            luzeena = len(batch[-1][0])\n",
        "            src1 = [sartu_padding(esaldia[0], luzeena) for esaldia in batch]\n",
        "            luzeena = max([len(esaldia[1]) for esaldia in batch])\n",
        "            src2 = [sartu_padding(esaldia[1], luzeena) for esaldia in batch]\n",
        "            src1 = torch.LongTensor(src1)\n",
        "            src2 = torch.LongTensor(src2)\n",
        "\n",
        "            src1_mask = (src1 != pad).unsqueeze(-2).cuda()\n",
        "            src2_mask = (src2 != pad).unsqueeze(-2).cuda()\n",
        "            emb1 = model.embed(src1.cuda())\n",
        "            e1_outputs = model.encoder(emb1, src1_mask)\n",
        "            emb2 = model.embed(src2.cuda())\n",
        "            e2_outputs = model.encoder(emb2, src2_mask)\n",
        "\n",
        "            # Lehenengo hitzerako aukerak lortu:\n",
        "\n",
        "            trg_mask = torch.tril(torch.ones(1, 1, 1, dtype=torch.uint8)).cuda()\n",
        "\n",
        "            hasierakoa = torch.full(\n",
        "                (len(src1), 1), sos, dtype=src1.dtype).cuda()\n",
        "            embed = model.embed(hasierakoa)\n",
        "            out = model.out(model.decoder(embed, \n",
        "                e1_outputs, e2_outputs, src1_mask, src2_mask, trg_mask))\n",
        "            out = F.softmax(out, dim=-1)\n",
        "            # Lerro berria:\n",
        "            out = out.cpu().float()\n",
        "\n",
        "            val, ix = out[:, -1].topk(k)\n",
        "\n",
        "            # Hurrengo hitzetarako begizta prestatu:\n",
        "\n",
        "            batch_size_dec = k*len(src1)\n",
        "            outputs = torch.zeros(\n",
        "                batch_size_dec, max_seq_len, dtype=src1.dtype)#.cuda()\n",
        "            outputs[:, 0] = torch.LongTensor([sos])\n",
        "            outputs[:, 1] = ix.flatten()\n",
        "            beam_prob = val.flatten().log()#.float()\n",
        "\n",
        "            src1_mask = src1_mask.repeat_interleave(k, dim=0)\n",
        "            src2_mask = src2_mask.repeat_interleave(k, dim=0)\n",
        "            e1_outputs = e1_outputs.repeat_interleave(k, dim=0)\n",
        "            e2_outputs = e2_outputs.repeat_interleave(k, dim=0)\n",
        "\n",
        "            bukatu_du = batch_size_dec*[False]\n",
        "            \n",
        "            berriro_saiatu = False\n",
        "\n",
        "            # Hurrengo hitzetarako begizta:\n",
        "\n",
        "            for i in range(2, max_seq_len):\n",
        "\n",
        "                if torch.cuda.memory_allocated(0) > 12_000_000_000 and \\\n",
        "                    batch_size_enc > 1:\n",
        "                    print(\"Memoria betetzen ari da.\")\n",
        "                    berriro_saiatu = True\n",
        "                    break\n",
        "\n",
        "                trg_mask = torch.tril(torch.ones(1, i, i, dtype=torch.uint8)).cuda()\n",
        "                embed = model.embed(outputs[:, :i].cuda())\n",
        "                out = model.decoder(embed, \n",
        "                    e1_outputs, e2_outputs, src1_mask, src2_mask, trg_mask)\n",
        "                out = model.out(out)\n",
        "                out = out[:, -1]\n",
        "                out = F.softmax(out, dim=-1)\n",
        "                out = out.log()\n",
        "                out = out.cpu()\n",
        "                out = out.float()\n",
        "\n",
        "                outputs_berria = torch.zeros_like(outputs)\n",
        "                bukatu_du_berria = batch_size_dec*[False]\n",
        "\n",
        "                for batch_ix in range(0, batch_size_dec, k):\n",
        "                    aukera_guztiak = torch.tensor([])\n",
        "                    for k_ix in range(k):\n",
        "                        if not bukatu_du[batch_ix+k_ix]:\n",
        "                            berriak = beam_prob[batch_ix+k_ix]+ \\\n",
        "                                out[batch_ix+k_ix]\n",
        "                            aukera_guztiak = torch.cat((aukera_guztiak, berriak))\n",
        "                        else:\n",
        "                            # Aukera hau <eos>era iritsi bada, ez dugu zabaldu nahi,\n",
        "                            # baina aukera bezala utzi nahi dugu. Beraz, behin\n",
        "                            # sartzen da, eta gainontzeko tokiak -inf-ekin betetzen\n",
        "                            # dira.\n",
        "                            berriak = torch.tensor(\n",
        "                            [beam_prob[batch_ix+k_ix]]+(vocab_size-1)*[-math.inf])\n",
        "                            aukera_guztiak = torch.cat((aukera_guztiak, berriak))\n",
        "\n",
        "                    val, ix = aukera_guztiak.topk(k)\n",
        "\n",
        "                    for k_ix in range(k):\n",
        "                        # zenbatgarren aukeratik datorren:\n",
        "                        ix_zahar = ix[k_ix] // vocab_size \n",
        "                        # hitz berriaren zenbakia:\n",
        "                        hitza = ix[k_ix] % vocab_size\n",
        "\n",
        "                        # bukatu_du eguneratu:\n",
        "                        if hitza == eos:\n",
        "                            bukatu_du_berria[batch_ix+k_ix] = True\n",
        "                        else:\n",
        "                            bukatu_du_berria[batch_ix+k_ix] = \\\n",
        "                                bukatu_du[batch_ix+ix_zahar]\n",
        "\n",
        "                        # Aurreko hitzak hartu:\n",
        "                        outputs_berria[batch_ix+k_ix] = outputs[batch_ix+ix_zahar]\n",
        "\n",
        "                        # Hitz berria gehitu:\n",
        "                        outputs_berria[batch_ix+k_ix, i] = hitza\n",
        "                        # Aukeren probabilitateak eguneratu:\n",
        "                        beam_prob[batch_ix+k_ix] = val[k_ix]\n",
        "\n",
        "                outputs = outputs_berria\n",
        "                bukatu_du = bukatu_du_berria\n",
        "\n",
        "                # Aukera guztiak bukatu badira, hurrengo batch-era pasa\n",
        "                if not (False in bukatu_du):\n",
        "                    break\n",
        "\n",
        "            if berriro_saiatu:\n",
        "                continue\n",
        "                    \n",
        "            # Aukera onena hartu eta idatzi:\n",
        "            #for esaldia in outputs:\n",
        "            for batch_ix in range(0, batch_size_dec, k):\n",
        "                max_ix = beam_prob[batch_ix : batch_ix+k].argmax()\n",
        "                onena = outputs[batch_ix + max_ix]\n",
        "                for pos in range(max_seq_len):\n",
        "                    if onena[pos] == eos:\n",
        "                        break\n",
        "                deskodetuta = bpe_hirurak.decode(onena[1:pos].tolist())[0]\n",
        "                ordenatuko_ix = j + batch_ix//k\n",
        "                jatorrizko_ix = src[ordenatuko_ix][2]\n",
        "                idazteko[jatorrizko_ix] = deskodetuta\n",
        "                \n",
        "            j = j + batch_size_enc\n",
        "        \n",
        "    print(f\"{time.time()-hasi} segundo behar izan ditu.\")\n",
        "    return idazteko"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GlDhUccuH3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adf04541-29e0-48a8-8e8b-122a5c13f51b"
      },
      "source": [
        "%cd $helbidea"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/HACOSDatuak\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQGyLE-A2vRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bpe_hirurak = yttm.BPE(model='bpe_hirurak.model')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klE0JQDN4hXI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bc67e41-7683-469c-f422-bac8d36eb8d0"
      },
      "source": [
        "model = SharedDualTransformer(vocab_size, d_model, N, heads)\n",
        "model.cuda()\n",
        "model = amp.initialize(model, opt_level='O2')\n",
        "model.load_state_dict(torch.load('parametroak.pt')['model'])\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SharedDualTransformer(\n",
              "  (embed): Embedder(\n",
              "    (embed): Embedding(20000, 512)\n",
              "  )\n",
              "  (encoder): SharedEncoder(\n",
              "    (pe): PositionalEncoder()\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): Norm()\n",
              "  )\n",
              "  (decoder): SharedDualDecoder(\n",
              "    (pe): PositionalEncoder()\n",
              "    (layers): ModuleList(\n",
              "      (0): DualDecoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (norm_3): Norm()\n",
              "        (norm_4): Norm()\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_4): Dropout(p=0.1, inplace=False)\n",
              "        (attn_1): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_2): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_3): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (1): DualDecoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (norm_3): Norm()\n",
              "        (norm_4): Norm()\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_4): Dropout(p=0.1, inplace=False)\n",
              "        (attn_1): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_2): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_3): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (2): DualDecoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (norm_3): Norm()\n",
              "        (norm_4): Norm()\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_4): Dropout(p=0.1, inplace=False)\n",
              "        (attn_1): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_2): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_3): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (3): DualDecoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (norm_3): Norm()\n",
              "        (norm_4): Norm()\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_4): Dropout(p=0.1, inplace=False)\n",
              "        (attn_1): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_2): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_3): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (4): DualDecoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (norm_3): Norm()\n",
              "        (norm_4): Norm()\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_4): Dropout(p=0.1, inplace=False)\n",
              "        (attn_1): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_2): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_3): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (5): DualDecoderLayer(\n",
              "        (norm_1): Norm()\n",
              "        (norm_2): Norm()\n",
              "        (norm_3): Norm()\n",
              "        (norm_4): Norm()\n",
              "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_3): Dropout(p=0.1, inplace=False)\n",
              "        (dropout_4): Dropout(p=0.1, inplace=False)\n",
              "        (attn_1): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_2): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (attn_3): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): Norm()\n",
              "  )\n",
              "  (out): Linear(in_features=512, out_features=20000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLtOLUg-6Dtk",
        "colab_type": "text"
      },
      "source": [
        "# Itzultzailea probatzeko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSQfEXMP6O0p",
        "colab_type": "text"
      },
      "source": [
        "Idatzi hurrengo gelaxkan gaztelaniazko eta ingelesezko testuak. Ondoren, exekutatu azpikoak. Euskarazko itzulpena behean agertuko da."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPtgqdg448Zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = \"Esto es un traductor de dos fuentes: pruébalo aquí.\"\n",
        "en = \"This is a dual source translator: try it here.\""
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf9tGrk16ne8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c66318e3-69cf-41db-b859-d1c9a28abe71"
      },
      "source": [
        "itzultzeko = [[es, en]]\n",
        "itzultzeko_tok = [[tokenizatu_str(jat[0]), tokenizatu_str(jat[1])] \n",
        "                  for jat in itzultzeko]\n",
        "\n",
        "itzultzeko_zenb = [[bpe_hirurak.encode(jat[0]), bpe_hirurak.encode(jat[1])] \n",
        "                   for jat in itzultzeko_tok]\n",
        "\n",
        "itzulia = itzuli_beam(model, itzultzeko_zenb, 4, batch_size_val)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "142761472\n",
            "1. esalditik aurrera itzultzen...\n",
            "0.2946932315826416 segundo behar izan ditu.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f95Xo4U7q7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be577679-fa9a-40ff-f576-dd46de3f97cd"
      },
      "source": [
        "for lerroa in itzulia:\n",
        "    lerroa = lerroa.replace('｟C', '｟mrk_case_modifier_C｠')\n",
        "    lerroa = lerroa.replace('｟B', '｟mrk_begin_case_region_U｠')\n",
        "    lerroa = lerroa.replace('｟E', '｟mrk_end_case_region_U｠')\n",
        "    print(destokenizer.detokenize(lerroa.split()))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hau bi iturrien itzultzaile bat da: proba ezazu hemen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKWRAo_LfTIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}